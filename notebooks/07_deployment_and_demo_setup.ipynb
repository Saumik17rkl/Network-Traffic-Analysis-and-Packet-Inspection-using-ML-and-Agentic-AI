{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ce487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ðŸš€ Deployment & Demo Setup\\n\\nThis notebook prepares the Streamlit demo and deployment artifacts, writes a production-ready `streamlit_app.py` skeleton, an `agent_fsm.py` module, a pinned `requirements.txt`, and checks model artifact sizes.\\n\\nOutputs:\\n- `streamlit_app.py`\\n- `agent_fsm.py`\\n- `requirements.txt`\\n- `deploy_instructions.md`\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# ðŸš€ Deployment & Demo Setup\n",
    "\n",
    "This notebook prepares the Streamlit demo and deployment artifacts, writes a production-ready `streamlit_app.py` skeleton, an `agent_fsm.py` module, a pinned `requirements.txt`, and checks model artifact sizes.\n",
    "\n",
    "Outputs:\n",
    "- `streamlit_app.py`\n",
    "- `agent_fsm.py`\n",
    "- `requirements.txt`\n",
    "- `deploy_instructions.md`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d07673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories verified.\n"
     ]
    }
   ],
   "source": [
    "import os, json, shutil\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "Path(\"artifacts/models\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"artifacts/eval\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"app\").mkdir(exist_ok=True)\n",
    "print(\"Directories verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cfd1795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifacts found:\n",
      "{\n",
      "  \"artifacts\\\\models\\\\flow_iforest_20251108_231711.joblib\": \"277.0KB\",\n",
      "  \"artifacts\\\\models\\\\flow_logreg_20251108_231711.joblib\": \"927.0B\",\n",
      "  \"artifacts\\\\models\\\\flow_rf_20251108_231711.joblib\": \"50.6KB\",\n",
      "  \"artifacts\\\\models\\\\packet_cnn_torch_v1.onnx\": \"23.9KB\",\n",
      "  \"artifacts\\\\models\\\\packet_cnn_torch_v1.onnx.data\": \"4.5KB\",\n",
      "  \"artifacts\\\\models\\\\packet_cnn_torch_v1.onnx.tmp\": \"19.6KB\",\n",
      "  \"artifacts\\\\models\\\\packet_cnn_torch_v1.onnx.tmp.data\": \"4.5KB\",\n",
      "  \"artifacts\\\\models\\\\packet_cnn_torch_v1.pt\": \"8.9KB\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def human_size(path):\n",
    "    sz = os.path.getsize(path)\n",
    "    for unit in ['B','KB','MB','GB']:\n",
    "        if sz < 1024:\n",
    "            return f\"{sz:.1f}{unit}\"\n",
    "        sz /= 1024\n",
    "    return f\"{sz:.1f}TB\"\n",
    "\n",
    "model_files = list(Path(\"artifacts/models\").glob(\"*\"))\n",
    "model_info = {str(p): human_size(str(p)) for p in model_files}\n",
    "print(\"Model artifacts found:\")\n",
    "print(json.dumps(model_info, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8456debd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Requirements\\nWe pin minimal CPU-only dependencies for Streamlit Cloud and local runs.\\n- `scikit-learn` for flow models\\n- `torch` (cpu) + `onnxruntime` for packet model inference\\n- `scapy` or `pyshark` for optional live capture (scapy recommended)\\n- `streamlit` for demo UI\\n- `matplotlib`, `pandas`, `numpy`, `joblib`, `tqdm`\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## Requirements\n",
    "We pin minimal CPU-only dependencies for Streamlit Cloud and local runs.\n",
    "- `scikit-learn` for flow models\n",
    "- `torch` (cpu) + `onnxruntime` for packet model inference\n",
    "- `scapy` or `pyshark` for optional live capture (scapy recommended)\n",
    "- `streamlit` for demo UI\n",
    "- `matplotlib`, `pandas`, `numpy`, `joblib`, `tqdm`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44da2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… requirements.txt written\n"
     ]
    }
   ],
   "source": [
    "reqs = [\n",
    "    \"python==3.10.*\",\n",
    "    \"streamlit>=1.18.0\",\n",
    "    \"scikit-learn>=1.2.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"pandas>=1.5.0\",\n",
    "    \"matplotlib>=3.6.0\",\n",
    "    \"joblib>=1.2.0\",\n",
    "    \"onnxruntime>=1.15.0\",\n",
    "    \"torch==2.0.1+cpu\",  # optional CPU wheel; adjust per environment\n",
    "    \"scapy>=2.5.0\",\n",
    "    \"tqdm>=4.64.0\"\n",
    "]\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(reqs))\n",
    "print(\"âœ… requirements.txt written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1dad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… agent_fsm.py written\n"
     ]
    }
   ],
   "source": [
    "agent_code = r'''\n",
    "# agent_fsm.py â€” deterministic FSM module for demo\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class AgentFSM:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or {\n",
    "            \"prob_threshold_investigate\": 0.7,\n",
    "            \"prob_threshold_report\": 0.85,\n",
    "            \"prob_threshold_contain\": 0.9,\n",
    "            \"persistence_window\": 2,\n",
    "            \"containment_duration\": 3\n",
    "        }\n",
    "        self.state = \"Monitoring\"\n",
    "        self.memory = []\n",
    "        self.containment_timer = 0\n",
    "        self.trace = []\n",
    "\n",
    "    def _log(self, alert, action, reason):\n",
    "        entry = {\n",
    "            \"time\": datetime.now().isoformat(),\n",
    "            \"state\": self.state,\n",
    "            \"packet_idx\": int(alert.get(\"packet_idx\", -1)),\n",
    "            \"prob\": float(alert.get(\"packet_base_prob\", alert.get(\"prob\",0.0))),\n",
    "            \"action\": action,\n",
    "            \"reason\": reason\n",
    "        }\n",
    "        self.trace.append(entry)\n",
    "        return entry\n",
    "\n",
    "    def transition(self, new_state, reason):\n",
    "        prev = self.state\n",
    "        self.state = new_state\n",
    "        return f\"{prev} -> {new_state} ({reason})\"\n",
    "\n",
    "    def handle_alert(self, alert):\n",
    "        prob = float(alert.get(\"packet_base_prob\", alert.get(\"prob\",0.0)))\n",
    "        self.memory.append({\"prob\": prob, \"packet_idx\": alert.get(\"packet_idx\")})\n",
    "        self.memory = self.memory[-self.config[\"persistence_window\"]:]\n",
    "        if self.state == \"Monitoring\":\n",
    "            if prob >= self.config[\"prob_threshold_investigate\"]:\n",
    "                msg = self.transition(\"Investigating\", \"High prob\")\n",
    "                return self._log(alert, \"investigate\", msg)\n",
    "            else:\n",
    "                return self._log(alert, \"monitor\", \"below threshold\")\n",
    "        elif self.state == \"Investigating\":\n",
    "            if all(m[\"prob\"] > self.config[\"prob_threshold_report\"] for m in self.memory):\n",
    "                msg = self.transition(\"Reporting\", \"persistent\")\n",
    "                return self._log(alert, \"report\", msg)\n",
    "            if prob < 0.5:\n",
    "                msg = self.transition(\"Monitoring\", \"normalized\")\n",
    "                return self._log(alert, \"monitor\", msg)\n",
    "            return self._log(alert, \"investigate\", \"continue\")\n",
    "        elif self.state == \"Reporting\":\n",
    "            if all(m[\"prob\"] > self.config[\"prob_threshold_contain\"] for m in self.memory):\n",
    "                msg = self.transition(\"Containment\", \"critical\")\n",
    "                self.containment_timer = self.config[\"containment_duration\"]\n",
    "                return self._log(alert, \"contain\", msg)\n",
    "            if prob < 0.6:\n",
    "                msg = self.transition(\"Monitoring\", \"false alarm\")\n",
    "                return self._log(alert, \"monitor\", msg)\n",
    "            return self._log(alert, \"report\", \"persistence\")\n",
    "        elif self.state == \"Containment\":\n",
    "            self.containment_timer -= 1\n",
    "            if self.containment_timer <= 0:\n",
    "                msg = self.transition(\"Monitoring\", \"release\")\n",
    "                return self._log(alert, \"release\", msg)\n",
    "            return self._log(alert, \"contain\", \"ongoing\")\n",
    "\n",
    "    def save_trace(self, path):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(self.trace, f, indent=2)\n",
    "'''\n",
    "with open(\"agent_fsm.py\", \"w\") as f:\n",
    "    f.write(agent_code)\n",
    "print(\"âœ… agent_fsm.py written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea30c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… streamlit_app.py written\n"
     ]
    }
   ],
   "source": [
    "streamlit_code = r'''\n",
    "# streamlit_app.py â€” demo app for CompactNetTrace\n",
    "import streamlit as st\n",
    "import joblib, json, os, time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from agent_fsm import AgentFSM\n",
    "\n",
    "st.set_page_config(layout=\"wide\", page_title=\"CompactNetTrace Demo\")\n",
    "\n",
    "st.title(\"CompactNetTrace â€” Lightweight NTA + Agentic AI Demo\")\n",
    "\n",
    "# Sidebar: model selection / demo controls\n",
    "st.sidebar.header(\"Demo Controls\")\n",
    "use_sample = st.sidebar.checkbox(\"Use sample data (data/sample)\", value=True)\n",
    "stream_speed = st.sidebar.slider(\"Stream speed (s per alert)\", 0.2, 2.0, 0.6)\n",
    "\n",
    "# Load models\n",
    "MODEL_DIR = \"artifacts/models\"\n",
    "# find joblib flow model (pick latest)\n",
    "flow_models = [f for f in os.listdir(MODEL_DIR) if f.startswith(\"flow_rf\") or f.startswith(\"flow_logreg\")]\n",
    "flow_model = None\n",
    "if flow_models:\n",
    "    flow_model = joblib.load(os.path.join(MODEL_DIR, sorted(flow_models)[-1]))\n",
    "\n",
    "onnx_path = os.path.join(MODEL_DIR, \"packet_cnn_torch_v1.onnx\")\n",
    "onnx_sess = None\n",
    "if os.path.exists(onnx_path):\n",
    "    onnx_sess = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "    input_name = onnx_sess.get_inputs()[0].name\n",
    "\n",
    "st.sidebar.markdown(\"**Models loaded:**\")\n",
    "if flow_model: st.sidebar.write(f\"- Flow model: {type(flow_model).__name__}\")\n",
    "if onnx_sess: st.sidebar.write(\"- Packet ONNX model loaded\")\n",
    "\n",
    "# Alerts source\n",
    "ALERTS_PATH = \"artifacts/eval/example_alerts.json\"\n",
    "if use_sample and os.path.exists(ALERTS_PATH):\n",
    "    with open(ALERTS_PATH) as f:\n",
    "        alerts = json.load(f)\n",
    "else:\n",
    "    alerts = []\n",
    "\n",
    "# Agent\n",
    "agent = AgentFSM()\n",
    "\n",
    "# Main layout\n",
    "col1, col2 = st.columns([2,1])\n",
    "\n",
    "with col1:\n",
    "    st.header(\"Agent Timeline\")\n",
    "    timeline_placeholder = st.empty()\n",
    "with col2:\n",
    "    st.header(\"Agent Log\")\n",
    "    log_placeholder = st.empty()\n",
    "\n",
    "# Simulate streaming of alerts\n",
    "for i, alert in enumerate(alerts):\n",
    "    # Normalize keys for agent\n",
    "    if \"packet_base_prob\" in alert:\n",
    "        alert[\"prob\"] = alert[\"packet_base_prob\"]\n",
    "    # agent handles alert\n",
    "    log_entry = agent.handle_alert(alert)\n",
    "    agent.save_trace(\"artifacts/agent_trace.json\")\n",
    "\n",
    "    # update agent log\n",
    "    df = None\n",
    "    try:\n",
    "        with open(\"artifacts/agent_trace.json\") as f:\n",
    "            df = json.load(f)\n",
    "    except:\n",
    "        df = agent.trace\n",
    "    log_placeholder.dataframe(df[-10:][::-1])\n",
    "\n",
    "    # update timeline visualization (simple: use matplotlib via pyplot)\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.patches import Rectangle\n",
    "    state_order = {\"Monitoring\":0,\"Investigating\":1,\"Reporting\":2,\"Containment\":3}\n",
    "    state_colors = {\"Monitoring\":\"#2ecc71\",\"Investigating\":\"#f1c40f\",\"Reporting\":\"#3498db\",\"Containment\":\"#e74c3c\"}\n",
    "    trace_local = df\n",
    "    times = list(range(len(trace_local)))\n",
    "    states = [state_order[e[\"state\"]] for e in trace_local]\n",
    "    packets = [e.get(\"packet_idx\") for e in trace_local]\n",
    "    probs = [e.get(\"prob\", 0.0) for e in trace_local]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    for st_name, y in state_order.items():\n",
    "        ax.add_patch(Rectangle((-0.5, y - 0.5), len(trace_local) + 1, 1, color=state_colors[st_name], alpha=0.06))\n",
    "    for j in range(len(trace_local)):\n",
    "        color = list(state_colors.values())[states[j]]\n",
    "        ax.scatter(j, states[j], color=color, edgecolor='black', s=80)\n",
    "        if j>0:\n",
    "            ax.plot([j-1,j],[states[j-1],states[j]], color=color, alpha=0.5)\n",
    "        ax.text(j, states[j]+0.12, f\"{packets[j]} ({probs[j]:.2f})\", ha='center', fontsize=8)\n",
    "    ax.set_yticks(list(state_order.values())); ax.set_yticklabels(list(state_order.keys()))\n",
    "    ax.set_xlim(-0.5, max(5, len(trace_local)-1 + 0.5))\n",
    "    ax.set_ylim(-0.5,3.5)\n",
    "    ax.set_xlabel(\"Alert index\")\n",
    "    timeline_placeholder.pyplot(fig)\n",
    "\n",
    "    time.sleep(stream_speed)\n",
    "\n",
    "st.success(\"Stream finished (sample).\")\n",
    "'''\n",
    "with open(\"streamlit_app.py\", \"w\") as f:\n",
    "    f.write(streamlit_code)\n",
    "print(\"âœ… streamlit_app.py written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16984d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… deploy_instructions.md written\n"
     ]
    }
   ],
   "source": [
    "deploy_md = r'''\n",
    "# Deploying CompactNetTrace Demo (Streamlit Cloud)\n",
    "\n",
    "1. Commit the repository to GitHub.\n",
    "2. Ensure model artifacts are under `artifacts/models/` and the repo size < 100 MB. If models >100MB, use Git LFS or reduce size.\n",
    "3. In Streamlit Cloud (https://share.streamlit.io), create a new app, connect your GitHub repo, and set the main file to `streamlit_app.py`.\n",
    "4. Use the default settings (no secrets required).\n",
    "5. If app fails due to dependency versions, adjust `requirements.txt` and redeploy.\n",
    "\n",
    "Alternative: Hugging Face Spaces (Streamlit) â€” similar steps; ensure `runtime.txt` if needed.\n",
    "\n",
    "Note: Streamlit Cloud ephemeral filesystem â€” model files must be in the repo or downloaded during startup. For large models, host them externally and download at app start.\n",
    "'''\n",
    "with open(\"deploy_instructions.md\", \"w\") as f:\n",
    "    f.write(deploy_md)\n",
    "print(\"âœ… deploy_instructions.md written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb109d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## âœ… Deployment Checklist (produced)\\n- agent_fsm.py\\n- streamlit_app.py\\n- requirements.txt\\n- deploy_instructions.md\\n\\nNext: push to GitHub and follow `deploy_instructions.md` to deploy on Streamlit Cloud.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## âœ… Deployment Checklist (produced)\n",
    "- agent_fsm.py\n",
    "- streamlit_app.py\n",
    "- requirements.txt\n",
    "- deploy_instructions.md\n",
    "\n",
    "Next: push to GitHub and follow `deploy_instructions.md` to deploy on Streamlit Cloud.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d699f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
