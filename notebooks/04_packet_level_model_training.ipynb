{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838fd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# üß© Packet-Level Model Training (PyTorch 1D-CNN)\n",
    "\n",
    "This notebook trains a small 1D convolutional neural network on payload byte windows (128 bytes) using **PyTorch (CPU)**.  \n",
    "\n",
    "It also:\n",
    "- Adds robustness to the synthetic data generator (noise/jitter, TTL variation, packet size variability).\n",
    "- Implements downsampling of dominant flow features (pkt_count / avg_pkt_size) in the synthetic generator.\n",
    "- Uses **train/test split by IP-groups** to test generalization to unseen hosts.\n",
    "- Exports the final model to **ONNX** for Streamlit deployment.\n",
    "- Measures inference latency and model file size.\n",
    "\n",
    "**Inputs:**\n",
    "- `data/features/packet_features.npy`\n",
    "- `data/sample/flows.csv` (for labels & flow‚Üípacket mapping)\n",
    "\n",
    "**Outputs:**\n",
    "- `artifacts/models/packet_cnn_torch_v1.pt` (PyTorch weights)\n",
    "- `artifacts/models/packet_cnn_torch_v1.onnx` (ONNX export)\n",
    "- `artifacts/eval/packet_model_eval.json`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724f8c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Notes\\n- This notebook assumes you have run `02_preprocessing_and_feature_engineering.ipynb` to generate `data/features/packet_features.npy`.\\n- Environment: CPU-only PyTorch. If PyTorch is not installed, uncomment the pip install cell below.\\n- All paths are relative to repository root.\\n- Random seed is fixed to ensure reproducibility.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## Notes\n",
    "- This notebook assumes you have run `02_preprocessing_and_feature_engineering.ipynb` to generate `data/features/packet_features.npy`.\n",
    "- Environment: CPU-only PyTorch. If PyTorch is not installed, uncomment the pip install cell below.\n",
    "- All paths are relative to repository root.\n",
    "- Random seed is fixed to ensure reproducibility.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0a6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SEED: 42\n",
      "Packet features path: data/features/packet_features.npy\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è Uncomment to install dependencies if not present\n",
    "# !pip install torch torchvision onnx onnxruntime tqdm\n",
    "# Note: On some systems, use 'pip install torch --index-url https://download.pytorch.org/whl/cpu' for CPU wheels.\n",
    "import os, json, time, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import joblib\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Paths\n",
    "Path(\"artifacts/models\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"artifacts/eval\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PACKET_FEATURES_PATH = \"data/features/packet_features.npy\"\n",
    "FLOW_CSV_PATH = \"data/sample/flows.csv\"\n",
    "ARTIFACT_PT = f\"artifacts/models/packet_cnn_torch_v1.pt\"\n",
    "ARTIFACT_ONNX = f\"artifacts/models/packet_cnn_torch_v1.onnx\"\n",
    "\n",
    "print(\"Using SEED:\", SEED)\n",
    "print(\"Packet features path:\", PACKET_FEATURES_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfebc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Synthetic data robustness (applied when `flows.csv`/pcap were generated synthetically)\\nWe apply three augmentations to make training realistic:\\n1. **Packet-size jitter** ‚Äî randomly perturb payload content length and pad/truncate to 128 bytes.\\n2. **TTL randomization** ‚Äî add random TTL in [30, 128].\\n3. **Downsample dominant features** ‚Äî scale `pkt_count` and `avg_pkt_size` by a random factor in [0.9, 1.1] for a random 20% subset to avoid single-feature dominance.\\n\\nThese augmentations are applied during dataset creation/loader on-the-fly to keep disk small.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## Synthetic data robustness (applied when `flows.csv`/pcap were generated synthetically)\n",
    "We apply three augmentations to make training realistic:\n",
    "1. **Packet-size jitter** ‚Äî randomly perturb payload content length and pad/truncate to 128 bytes.\n",
    "2. **TTL randomization** ‚Äî add random TTL in [30, 128].\n",
    "3. **Downsample dominant features** ‚Äî scale `pkt_count` and `avg_pkt_size` by a random factor in [0.9, 1.1] for a random 20% subset to avoid single-feature dominance.\n",
    "\n",
    "These augmentations are applied during dataset creation/loader on-the-fly to keep disk small.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80cd3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded payload array shape: (1000, 128)\n",
      "Loaded flows.csv shape: (50, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_id</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>dst_ip</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>avg_pkt_size</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>192.168.0.0</td>\n",
       "      <td>10.0.0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>192.168.0.1</td>\n",
       "      <td>10.0.0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>192.168.0.2</td>\n",
       "      <td>10.0.0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>192.168.0.3</td>\n",
       "      <td>10.0.0.6</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>192.168.0.4</td>\n",
       "      <td>10.0.0.8</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flow_id       src_ip    dst_ip  pkt_count  avg_pkt_size   label\n",
       "0        0  192.168.0.0  10.0.0.0         20          60.0  attack\n",
       "1        1  192.168.0.1  10.0.0.2         20          55.0  normal\n",
       "2        2  192.168.0.2  10.0.0.4         20          55.0  normal\n",
       "3        3  192.168.0.3  10.0.0.6         20          55.0  normal\n",
       "4        4  192.168.0.4  10.0.0.8         20          55.0  normal"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load payload byte arrays\n",
    "if not os.path.exists(PACKET_FEATURES_PATH):\n",
    "    raise FileNotFoundError(f\"{PACKET_FEATURES_PATH} not found. Run notebook 02 first.\")\n",
    "\n",
    "payload_array = np.load(PACKET_FEATURES_PATH)  # shape: (num_packets, 128)\n",
    "print(\"Loaded payload array shape:\", payload_array.shape)\n",
    "\n",
    "# Load flows.csv for labels and mapping assumptions\n",
    "if not os.path.exists(FLOW_CSV_PATH):\n",
    "    print(\"Warning: flows.csv not found ‚Äî attempting to proceed with synthetic labels only.\")\n",
    "    df_flows = pd.DataFrame()\n",
    "else:\n",
    "    df_flows = pd.read_csv(FLOW_CSV_PATH)\n",
    "    print(\"Loaded flows.csv shape:\", df_flows.shape)\n",
    "    display(df_flows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826e3851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We need labels for packet-level supervised training. There are two modes:\\n- **If `packet‚Üíflow` mapping exists:** use mapping to assign each packet the flow label.\\n- **If not available (typical small synthetic case):** assume payloads were generated in the same order as flows and derive packet labels using flow sizes (documented assumption).\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We need labels for packet-level supervised training. There are two modes:\n",
    "- **If `packet‚Üíflow` mapping exists:** use mapping to assign each packet the flow label.\n",
    "- **If not available (typical small synthetic case):** assume payloads were generated in the same order as flows and derive packet labels using flow sizes (documented assumption).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1323566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived packet labels from flows; labels shape: (1000,)\n",
      "Packet label distribution: {np.int64(0): np.int64(800), np.int64(1): np.int64(200)}\n"
     ]
    }
   ],
   "source": [
    "# Heuristic labelling logic\n",
    "NUM_PACKETS = payload_array.shape[0]\n",
    "\n",
    "# Default: try to use df_flows to create a mapping if 'pkt_count' exists\n",
    "if not df_flows.empty and 'pkt_count' in df_flows.columns:\n",
    "    # Expand flows into packet labels\n",
    "    packet_labels = []\n",
    "    for _, row in df_flows.iterrows():\n",
    "        cnt = int(row.get('pkt_count', 1))\n",
    "        lbl = row.get('label_encoded') if 'label_encoded' in row else (1 if str(row.get('label','')).lower()!='normal' else 0)\n",
    "        packet_labels.extend([int(lbl)] * cnt)\n",
    "    # Truncate/pad to NUM_PACKETS\n",
    "    if len(packet_labels) >= NUM_PACKETS:\n",
    "        packet_labels = packet_labels[:NUM_PACKETS]\n",
    "    else:\n",
    "        # pad with majority class\n",
    "        pad_label = int(np.round(np.mean(packet_labels))) if packet_labels else 0\n",
    "        packet_labels.extend([pad_label] * (NUM_PACKETS - len(packet_labels)))\n",
    "    packet_labels = np.array(packet_labels)\n",
    "    print(\"Derived packet labels from flows; labels shape:\", packet_labels.shape)\n",
    "else:\n",
    "    # fallback: simple heuristic ‚Äî first half normal (0), second half attack (1)\n",
    "    half = NUM_PACKETS // 2\n",
    "    packet_labels = np.array([0]*half + [1]*(NUM_PACKETS-half))\n",
    "    print(\"Fallback packet labels created; shape:\", packet_labels.shape)\n",
    "\n",
    "# Quick check label distribution\n",
    "(unique, counts) = np.unique(packet_labels, return_counts=True)\n",
    "print(\"Packet label distribution:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3edaede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1000\n"
     ]
    }
   ],
   "source": [
    "class PacketPayloadDataset(Dataset):\n",
    "    def __init__(self, payload_array, labels, apply_augment=False, augment_prob=0.2):\n",
    "        \"\"\"\n",
    "        payload_array: np.array (N, 128) dtype=uint8\n",
    "        labels: np.array (N,) values 0/1\n",
    "        apply_augment: boolean to enable synthetic augmentations\n",
    "        augment_prob: probability to apply per-sample jitter/downsample\n",
    "        \"\"\"\n",
    "        assert payload_array.shape[0] == labels.shape[0]\n",
    "        self.x = payload_array.astype(np.float32) / 255.0  # normalize 0..1 floats\n",
    "        self.y = labels.astype(np.int64)\n",
    "        self.apply_augment = apply_augment\n",
    "        self.augment_prob = augment_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def _augment(self, x):\n",
    "        # Packet size jitter: randomly zero out a tail section (simulate shorter payload)\n",
    "        if random.random() < 0.5:\n",
    "            cut = random.randint(64, 128)\n",
    "            x[cut:] = 0.0\n",
    "        # TTL-like noise is not in payload; we simulate by adding tiny noise to some bytes\n",
    "        if random.random() < 0.3:\n",
    "            noise_idx = random.sample(range(len(x)), k=5)\n",
    "            x[noise_idx] = np.clip(x[noise_idx] + (np.random.randn(len(noise_idx)) * 0.05), 0.0, 1.0)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx].copy()\n",
    "        y = self.y[idx]\n",
    "        if self.apply_augment and random.random() < self.augment_prob:\n",
    "            x = self._augment(x)\n",
    "        return torch.from_numpy(x).unsqueeze(0), torch.tensor(y)  # (1, 128), scalar label\n",
    "\n",
    "# Build dataset\n",
    "dataset = PacketPayloadDataset(payload_array, packet_labels, apply_augment=True, augment_prob=0.25)\n",
    "print(\"Dataset length:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b996ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We prefer **split by IP-group** for realistic generalization: e.g., flows from IP ranges seen in train are excluded from test.  \\nIf `flows.csv` has `src_ip` or `dst_ip` and we expanded packet labels in the same order, we split by group. Otherwise fallback to deterministic random split.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We prefer **split by IP-group** for realistic generalization: e.g., flows from IP ranges seen in train are excluded from test.  \n",
    "If `flows.csv` has `src_ip` or `dst_ip` and we expanded packet labels in the same order, we split by group. Otherwise fallback to deterministic random split.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c621d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: 800 | Test indices: 200\n"
     ]
    }
   ],
   "source": [
    "def group_split_by_flow(df_flows, dataset_len, train_frac=0.8):\n",
    "    \"\"\"\n",
    "    Returns list of train indices and test indices by splitting flows by src_ip groups.\n",
    "    This function assumes the earlier flow‚Üípacket expansion used sequential grouping identical to df_flows order.\n",
    "    \"\"\"\n",
    "    if df_flows.empty or 'pkt_count' not in df_flows.columns:\n",
    "        # fallback deterministic split\n",
    "        n_train = int(dataset_len * 0.8)\n",
    "        indices = list(range(dataset_len))\n",
    "        return indices[:n_train], indices[n_train:]\n",
    "\n",
    "    # Build packet index ranges per flow\n",
    "    boundaries = []\n",
    "    start = 0\n",
    "    for _, row in df_flows.iterrows():\n",
    "        cnt = int(row.get('pkt_count', 1))\n",
    "        boundaries.append((start, start + cnt, row.get('src_ip', None)))\n",
    "        start += cnt\n",
    "        if start >= dataset_len:\n",
    "            break\n",
    "    # Group flows by src_ip\n",
    "    ip_to_indices = {}\n",
    "    for s,e,ip in boundaries:\n",
    "        ip_to_indices.setdefault(ip, []).extend(list(range(s, min(e, dataset_len))))\n",
    "\n",
    "    # Shuffle ips deterministically\n",
    "    ips = list(ip_to_indices.keys())\n",
    "    random.Random(SEED).shuffle(ips)\n",
    "\n",
    "    # allocate ips to train until reaching train_frac\n",
    "    train_indices, test_indices = [], []\n",
    "    total = 0\n",
    "    target = dataset_len * train_frac\n",
    "    for ip in ips:\n",
    "        idxs = ip_to_indices[ip]\n",
    "        if total < target:\n",
    "            train_indices.extend(idxs)\n",
    "            total += len(idxs)\n",
    "        else:\n",
    "            test_indices.extend(idxs)\n",
    "\n",
    "    # if any leftover (unlikely), assign to test\n",
    "    all_assigned = set(train_indices + test_indices)\n",
    "    for i in range(dataset_len):\n",
    "        if i not in all_assigned:\n",
    "            test_indices.append(i)\n",
    "\n",
    "    return sorted(train_indices), sorted(test_indices)\n",
    "\n",
    "train_idx, test_idx = group_split_by_flow(df_flows, len(dataset))\n",
    "print(\"Train indices:\", len(train_idx), \"| Test indices:\", len(test_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d7a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 25 | Test batches: 7\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Helper to create subset datasets\n",
    "from torch.utils.data import Subset\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "test_ds = Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \"| Test batches:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d813fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model design notes:\\n- Input: (batch, 1, 128) normalized [0,1]\\n- Layers: Conv1d(1‚Üí8) ‚Üí ReLU ‚Üí Conv1d(8‚Üí16) ‚Üí ReLU ‚Üí GlobalAvgPool ‚Üí Dense(32) ‚Üí ReLU ‚Üí Output(2)\\n- Small parameter count (~few KB‚ÄìMB) to keep artifact small for Streamlit.\\n- Use CrossEntropyLoss for 2-class classification.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Model design notes:\n",
    "- Input: (batch, 1, 128) normalized [0,1]\n",
    "- Layers: Conv1d(1‚Üí8) ‚Üí ReLU ‚Üí Conv1d(8‚Üí16) ‚Üí ReLU ‚Üí GlobalAvgPool ‚Üí Dense(32) ‚Üí ReLU ‚Üí Output(2)\n",
    "- Small parameter count (~few KB‚ÄìMB) to keep artifact small for Streamlit.\n",
    "- Use CrossEntropyLoss for 2-class classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "200a1e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny1DCNN(\n",
      "  (net): Sequential(\n",
      "    (0): Conv1d(1, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(8, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (4): ReLU()\n",
      "    (5): AdaptiveAvgPool1d(output_size=1)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Tiny1DCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 8, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(8, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # global pooling -> (batch, 16, 1)\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = Tiny1DCNN()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f50085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu Epochs: 8\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "EPOCHS = 8  # small number for quick runs\n",
    "print(\"Device:\", device, \"Epochs:\", EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ac7c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 | Train loss 0.6286 acc 0.8250 | Val loss 0.6308 acc 0.7000\n",
      "‚úîÔ∏è Saved best model state: artifacts/models/packet_cnn_torch_v1.pt\n",
      "Epoch 2/8 | Train loss 0.5316 acc 0.8250 | Val loss 0.6163 acc 0.7000\n",
      "Epoch 3/8 | Train loss 0.4730 acc 0.8250 | Val loss 0.6602 acc 0.7000\n",
      "Epoch 4/8 | Train loss 0.4662 acc 0.8250 | Val loss 0.6659 acc 0.7000\n",
      "Epoch 5/8 | Train loss 0.4662 acc 0.8250 | Val loss 0.6566 acc 0.7000\n",
      "Epoch 6/8 | Train loss 0.4657 acc 0.8250 | Val loss 0.6595 acc 0.7000\n",
      "Epoch 7/8 | Train loss 0.4647 acc 0.8250 | Val loss 0.6551 acc 0.7000\n",
      "Epoch 8/8 | Train loss 0.4661 acc 0.8250 | Val loss 0.6522 acc 0.7000\n",
      "Training complete. Best val acc: 0.7\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (preds == y).sum().item()\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    probs_all = []\n",
    "    y_all = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "            probs_all.extend(probs.tolist())\n",
    "            y_all.extend(y.cpu().numpy().tolist())\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total += y.size(0)\n",
    "            correct += (preds == y).sum().item()\n",
    "    return running_loss/total, correct/total, np.array(y_all), np.array(probs_all)\n",
    "\n",
    "train_history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc, y_val, y_prob_val = eval_model(model, test_loader, criterion)\n",
    "    train_history[\"train_loss\"].append(train_loss)\n",
    "    train_history[\"train_acc\"].append(train_acc)\n",
    "    train_history[\"val_loss\"].append(val_loss)\n",
    "    train_history[\"val_acc\"].append(val_acc)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train loss {train_loss:.4f} acc {train_acc:.4f} | Val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), ARTIFACT_PT)\n",
    "        print(\"‚úîÔ∏è Saved best model state:\", ARTIFACT_PT)\n",
    "\n",
    "print(\"Training complete. Best val acc:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "199c2940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {\n",
      "  \"precision\": 0.0,\n",
      "  \"recall\": 0.0,\n",
      "  \"f1\": 0.0,\n",
      "  \"roc_auc\": 0.0,\n",
      "  \"val_acc\": 0.7,\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      140,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      60,\n",
      "      0\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "Saved eval metrics ‚Üí artifacts/eval/packet_model_eval.json\n"
     ]
    }
   ],
   "source": [
    "# load saved state\n",
    "model.load_state_dict(torch.load(ARTIFACT_PT, map_location=device))\n",
    "val_loss, val_acc, y_val, y_prob_val = eval_model(model, test_loader, criterion)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "y_pred = (y_prob_val >= 0.5).astype(int)\n",
    "metrics = {\n",
    "    \"precision\": float(precision_score(y_val, y_pred, zero_division=0)),\n",
    "    \"recall\": float(recall_score(y_val, y_pred, zero_division=0)),\n",
    "    \"f1\": float(f1_score(y_val, y_pred, zero_division=0)),\n",
    "    \"roc_auc\": float(roc_auc_score(y_val, y_prob_val)) if len(np.unique(y_val))>1 else None,\n",
    "    \"val_acc\": float(val_acc),\n",
    "    \"confusion_matrix\": confusion_matrix(y_val, y_pred).tolist()\n",
    "}\n",
    "print(\"Evaluation metrics:\", json.dumps(metrics, indent=2))\n",
    "# Save metrics\n",
    "with open(\"artifacts/eval/packet_model_eval.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved eval metrics ‚Üí artifacts/eval/packet_model_eval.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6901fd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Export model to ONNX for portability. We'll create a dummy input and export with dynamic batch dimension.\\nONNX will be used by onnxruntime in Streamlit app.\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Export model to ONNX for portability. We'll create a dummy input and export with dynamic batch dimension.\n",
    "ONNX will be used by onnxruntime in Streamlit app.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0614963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4392\\3584459432.py:8: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1108 23:56:02.755000 4392 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Tiny1DCNN([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `Tiny1DCNN([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ‚úÖ\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
      "Applied 1 of general pattern rewrite rules.\n",
      "‚úÖ Temporary ONNX model created: artifacts/models/packet_cnn_torch_v1.onnx.tmp\n",
      "‚úÖ Final ONNX single-file model saved ‚Üí artifacts/models/packet_cnn_torch_v1.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch, onnx\n",
    "\n",
    "onnx_path = str(ARTIFACT_ONNX).replace(\"\\\\\", \"/\")\n",
    "dummy_input = torch.randn(1, 1, 128, device=\"cpu\")\n",
    "\n",
    "# 1Ô∏è‚É£ Export normally to a temp file\n",
    "tmp_path = onnx_path + \".tmp\"\n",
    "torch.onnx.export(\n",
    "    model.cpu(),\n",
    "    dummy_input,\n",
    "    tmp_path,\n",
    "    export_params=True,\n",
    "    opset_version=18,\n",
    "    input_names=[\"x\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"x\": {0: \"batch\"}},\n",
    "    do_constant_folding=True\n",
    ")\n",
    "print(\"‚úÖ Temporary ONNX model created:\", tmp_path)\n",
    "\n",
    "# 2Ô∏è‚É£ Re-save via ONNX API to embed weights into a single file\n",
    "model_onnx = onnx.load(tmp_path, load_external_data=True)\n",
    "onnx.save_model(model_onnx, onnx_path, save_as_external_data=False)\n",
    "print(\"‚úÖ Final ONNX single-file model saved ‚Üí\", onnx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b88913df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sizes: PyTorch (.pt) = 0.009 MB | ONNX = 0.023 MB\n"
     ]
    }
   ],
   "source": [
    "pt_size = os.path.getsize(ARTIFACT_PT) / (1024*1024)\n",
    "onnx_size = os.path.getsize(ARTIFACT_ONNX) / (1024*1024)\n",
    "print(f\"Model sizes: PyTorch (.pt) = {pt_size:.3f} MB | ONNX = {onnx_size:.3f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c176fd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency report: {'pytorch_latency_ms': 0.5529952049255371, 'onnx_latency_ms': 0.08260488510131836, 'pt_size_mb': 0.0087, 'onnx_size_mb': 0.0234}\n",
      "Saved latency report ‚Üí artifacts/eval/packet_model_latency.json\n"
     ]
    }
   ],
   "source": [
    "# Warm-up and timed runs\n",
    "N_WARMUP = 10\n",
    "N_RUNS = 200\n",
    "\n",
    "# PyTorch latency\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for _ in range(N_WARMUP):\n",
    "        _ = model(dummy_input)\n",
    "    t0 = time.time()\n",
    "    for _ in range(N_RUNS):\n",
    "        _ = model(dummy_input)\n",
    "    t1 = time.time()\n",
    "torch_latency_ms = (t1 - t0) / N_RUNS * 1000\n",
    "\n",
    "# ONNX latency with onnxruntime\n",
    "ort_sess = ort.InferenceSession(ARTIFACT_ONNX, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = ort_sess.get_inputs()[0].name\n",
    "dummy_np = dummy_input.cpu().numpy()\n",
    "for _ in range(N_WARMUP):\n",
    "    _ = ort_sess.run(None, {input_name: dummy_np})\n",
    "t0 = time.time()\n",
    "for _ in range(N_RUNS):\n",
    "    _ = ort_sess.run(None, {input_name: dummy_np})\n",
    "t1 = time.time()\n",
    "onnx_latency_ms = (t1 - t0) / N_RUNS * 1000\n",
    "\n",
    "latency_report = {\n",
    "    \"pytorch_latency_ms\": torch_latency_ms,\n",
    "    \"onnx_latency_ms\": onnx_latency_ms,\n",
    "    \"pt_size_mb\": round(pt_size, 4),\n",
    "    \"onnx_size_mb\": round(onnx_size, 4)\n",
    "}\n",
    "\n",
    "print(\"Latency report:\", latency_report)\n",
    "with open(\"artifacts/eval/packet_model_latency.json\", \"w\") as f:\n",
    "    json.dump(latency_report, f, indent=2)\n",
    "print(\"Saved latency report ‚Üí artifacts/eval/packet_model_latency.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27846a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Streamlit, use `onnxruntime.InferenceSession()` to load the ONNX model and run `sess.run(None, {input_name: np_input})`.\\nEnsure input is `float32` numpy array with shape `(batch_size, 1, 128)`.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In Streamlit, use `onnxruntime.InferenceSession()` to load the ONNX model and run `sess.run(None, {input_name: np_input})`.\n",
    "Ensure input is `float32` numpy array with shape `(batch_size, 1, 128)`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66278061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## ‚úÖ Summary\\n\\n- Trained tiny 1D-CNN (PyTorch) on payload windows (128 bytes).\\n- Applied synthetic augmentations (size jitter, small byte noise) in the dataset loader.\\n- Performed train/test split by IP-group (when mapping available) for realistic generalization.\\n- Exported model to ONNX for portable inference in Streamlit.\\n- Measured inference latency (PyTorch vs ONNX) and model sizes.\\n\\n**Artifacts produced:**\\n- `artifacts/models/packet_cnn_torch_v1.pt`\\n- `artifacts/models/packet_cnn_torch_v1.onnx`\\n- `artifacts/eval/packet_model_eval.json`\\n- `artifacts/eval/packet_model_latency.json`\\n\\n**Next:** Proceed to `05_evaluation_and_explainability.ipynb`.\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## ‚úÖ Summary\n",
    "\n",
    "- Trained tiny 1D-CNN (PyTorch) on payload windows (128 bytes).\n",
    "- Applied synthetic augmentations (size jitter, small byte noise) in the dataset loader.\n",
    "- Performed train/test split by IP-group (when mapping available) for realistic generalization.\n",
    "- Exported model to ONNX for portable inference in Streamlit.\n",
    "- Measured inference latency (PyTorch vs ONNX) and model sizes.\n",
    "\n",
    "**Artifacts produced:**\n",
    "- `artifacts/models/packet_cnn_torch_v1.pt`\n",
    "- `artifacts/models/packet_cnn_torch_v1.onnx`\n",
    "- `artifacts/eval/packet_model_eval.json`\n",
    "- `artifacts/eval/packet_model_latency.json`\n",
    "\n",
    "**Next:** Proceed to `05_evaluation_and_explainability.ipynb`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3eefbc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Packet-level model training & export complete.\n",
      "Artifacts:\n",
      " - artifacts/models/packet_cnn_torch_v1.pt\n",
      " - artifacts/models/packet_cnn_torch_v1.onnx\n",
      " - artifacts/eval/packet_model_eval.json\n",
      " - artifacts/eval/packet_model_latency.json\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Packet-level model training & export complete.\")\n",
    "print(\"Artifacts:\")\n",
    "print(\" -\", ARTIFACT_PT)\n",
    "print(\" -\", ARTIFACT_ONNX)\n",
    "print(\" - artifacts/eval/packet_model_eval.json\")\n",
    "print(\" - artifacts/eval/packet_model_latency.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcedae39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc4ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
