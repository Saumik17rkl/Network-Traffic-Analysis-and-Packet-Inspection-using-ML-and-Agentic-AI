{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf1850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# üìä Data Acquisition & Sampling  \\nThis notebook loads **lightweight network traffic datasets** and prepares **small, reproducible subsets** for hybrid flow‚Äì and packet‚Äìlevel analysis.  \\n\\nWe support three data modes:\\n1. **Public datasets** (NSL-KDD, UNSW-NB15, CICIDS2017, N-BaIoT)  \\n2. **Synthetic dataset generation** (using Scapy recipe)  \\n3. **Live capture (optional)** ‚Äì for real-time experiments  \\n\\nAt the end of this notebook:\\n- `/data/sample/flows.csv` ‚Üí cleaned flow-level data  \\n- `/data/sample/packets.pcap` ‚Üí small packet-level traces  \\n- `/data/sample/metadata.json` ‚Üí reproducibility metadata  \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# üìä Data Acquisition & Sampling  \n",
    "This notebook loads **lightweight network traffic datasets** and prepares **small, reproducible subsets** for hybrid flow‚Äì and packet‚Äìlevel analysis.  \n",
    "\n",
    "We support three data modes:\n",
    "1. **Public datasets** (NSL-KDD, UNSW-NB15, CICIDS2017, N-BaIoT)  \n",
    "2. **Synthetic dataset generation** (using Scapy recipe)  \n",
    "3. **Live capture (optional)** ‚Äì for real-time experiments  \n",
    "\n",
    "At the end of this notebook:\n",
    "- `/data/sample/flows.csv` ‚Üí cleaned flow-level data  \n",
    "- `/data/sample/packets.pcap` ‚Üí small packet-level traces  \n",
    "- `/data/sample/metadata.json` ‚Üí reproducibility metadata  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2407186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## üîÅ Inputs & Outputs\\n\\n**Inputs:**\\n- Dataset URLs (KaggleHub / GitHub links)  \\n- `extras/live_capture_config.yaml` (for optional live capture)  \\n- Random seed from `reproducibility_info.json`\\n\\n**Outputs:**\\n- `/data/sample/flows.csv`  \\n- `/data/sample/packets.pcap`  \\n- `/data/sample/dataset_summary.json`  \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## üîÅ Inputs & Outputs\n",
    "\n",
    "**Inputs:**\n",
    "- Dataset URLs (KaggleHub / GitHub links)  \n",
    "- `extras/live_capture_config.yaml` (for optional live capture)  \n",
    "- Random seed from `reproducibility_info.json`\n",
    "\n",
    "**Outputs:**\n",
    "- `/data/sample/flows.csv`  \n",
    "- `/data/sample/packets.pcap`  \n",
    "- `/data/sample/dataset_summary.json`  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45d07e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from requests->kagglehub) (2025.10.5)\n",
      "Requirement already satisfied: colorama in d:\\project\\network traffic analysis and packet inspection using ml and agentic ai\\.venv\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\Network Traffic Analysis and Packet Inspection using ML and Agentic AI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directories ready: data/raw, data/sample\n"
     ]
    }
   ],
   "source": [
    "import os, json, random, pandas as pd, numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# For Scapy synthetic generation and PCAP parsing\n",
    "from scapy.all import IP, TCP, UDP, DNS, DNSQR, Raw, wrpcap\n",
    "\n",
    "# KaggleHub for dataset imports\n",
    "import kagglehub\n",
    "\n",
    "# Directory setup\n",
    "Path(\"data/sample\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"data/raw\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Directories ready: data/raw, data/sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0298ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed: 42\n"
     ]
    }
   ],
   "source": [
    "# üé≤ Load reproducibility config\n",
    "with open(\"reproducibility_info.json\", \"r\") as f:\n",
    "    repro_info = json.load(f)\n",
    "random.seed(repro_info[\"seed\"])\n",
    "np.random.seed(repro_info[\"seed\"])\n",
    "print(\"Using seed:\", repro_info[\"seed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314528cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below are lightweight datasets suitable for this project.  \\nEach can be downloaded directly via KaggleHub (if credentials available) or manually from GitHub/official sources.\\n\\n| Dataset | KaggleHub Import | Size | Use |\\n|----------|------------------|------|-----|\\n| NSL-KDD | `kagglehub.dataset_download(\"hassan06/nslkdd\")` | ~18 MB | Flow-level baseline |\\n| UNSW-NB15 | `kagglehub.dataset_download(\"mrwellsdavid/unsw-nb15\")` | <100 MB | Modern labeled flows |\\n| CICIDS2017 | `kagglehub.dataset_download(\"hcavsi/cicids2017-dataset\")` | variable | Hybrid flow + packet |\\n| N-BaIoT | `kagglehub.dataset_download(\"mkashifn/nbaiot-dataset\")` | few MB | IoT anomaly analysis |\\n| TON-IoT | `kagglehub.dataset_download(\"programmer3/ton-iot-network-intrusion-dataset\")` | <50 MB | Agentic orchestration |\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## üåê Public Dataset Options\n",
    "\"\"\"Below are lightweight datasets suitable for this project.  \n",
    "Each can be downloaded directly via KaggleHub (if credentials available) or manually from GitHub/official sources.\n",
    "\n",
    "| Dataset | KaggleHub Import | Size | Use |\n",
    "|----------|------------------|------|-----|\n",
    "| NSL-KDD | `kagglehub.dataset_download(\"hassan06/nslkdd\")` | ~18 MB | Flow-level baseline |\n",
    "| UNSW-NB15 | `kagglehub.dataset_download(\"mrwellsdavid/unsw-nb15\")` | <100 MB | Modern labeled flows |\n",
    "| CICIDS2017 | `kagglehub.dataset_download(\"hcavsi/cicids2017-dataset\")` | variable | Hybrid flow + packet |\n",
    "| N-BaIoT | `kagglehub.dataset_download(\"mkashifn/nbaiot-dataset\")` | few MB | IoT anomaly analysis |\n",
    "| TON-IoT | `kagglehub.dataset_download(\"programmer3/ton-iot-network-intrusion-dataset\")` | <50 MB | Agentic orchestration |\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9607cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset choice set to: NSL-KDD\n"
     ]
    }
   ],
   "source": [
    "# üîß Choose dataset: 'NSL-KDD', 'UNSW-NB15', 'CICIDS2017', 'N-BaIoT', or 'synthetic'\n",
    "DATASET_CHOICE = \"NSL-KDD\"\n",
    "\n",
    "print(f\"Dataset choice set to: {DATASET_CHOICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f88e01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Downloading NSL-KDD dataset...\n",
      "‚úÖ Dataset NSL-KDD downloaded and saved to: data\\raw\\NSL-KDD\n",
      "üìÅ Dataset available at: data\\raw\\NSL-KDD\n",
      "\n",
      "üìÇ Downloaded files:\n",
      "  - index.html\n",
      "  - KDDTest+.arff\n",
      "  - KDDTest+.txt\n",
      "  - KDDTest-21.arff\n",
      "  - KDDTest-21.txt\n",
      "  - KDDTest1.jpg\n",
      "  - KDDTrain+.arff\n",
      "  - KDDTrain+.txt\n",
      "  - KDDTrain+_20Percent.arff\n",
      "  - KDDTrain+_20Percent.txt\n",
      "  - KDDTrain1.jpg\n",
      "  - nsl-kdd\n"
     ]
    }
   ],
   "source": [
    "def download_dataset(choice, target_dir=\"data/raw\"):\n",
    "    \"\"\"\n",
    "    Download a dataset from KaggleHub and save it to the target directory.\n",
    "    \n",
    "    Args:\n",
    "        choice (str): Dataset name (e.g., \"NSL-KDD\", \"UNSW-NB15\", etc.)\n",
    "        target_dir (str): Directory to save the downloaded dataset (default: \"data/raw\")\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the downloaded dataset directory\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create target directory if it doesn't exist\n",
    "    target_path = Path(target_dir) / choice\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        print(f\"‚è≥ Downloading {choice} dataset...\")\n",
    "        \n",
    "        # Download the dataset\n",
    "        if choice == \"NSL-KDD\":\n",
    "            src_path = Path(kagglehub.dataset_download(\"hassan06/nslkdd\"))\n",
    "        elif choice == \"UNSW-NB15\":\n",
    "            src_path = Path(kagglehub.dataset_download(\"mrwellsdavid/unsw-nb15\"))\n",
    "        elif choice == \"CICIDS2017\":\n",
    "            src_path = Path(kagglehub.dataset_download(\"hcavsi/cicids2017-dataset\"))\n",
    "        elif choice == \"N-BaIoT\":\n",
    "            src_path = Path(kagglehub.dataset_download(\"mkashifn/nbaiot-dataset\"))\n",
    "        elif choice == \"TON-IoT\":\n",
    "            src_path = Path(kagglehub.dataset_download(\"programmer3/ton-iot-network-intrusion-dataset\"))\n",
    "        else:\n",
    "            print(f\"‚ùå Unsupported dataset: {choice}\")\n",
    "            return None\n",
    "        \n",
    "        # Copy all files from source to target directory\n",
    "        for item in src_path.glob('*'):\n",
    "            if item.is_file():\n",
    "                shutil.copy2(item, target_path / item.name)\n",
    "            elif item.is_dir():\n",
    "                shutil.copytree(item, target_path / item.name, dirs_exist_ok=True)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset {choice} downloaded and saved to: {target_path}\")\n",
    "        return str(target_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading {choice}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Download the dataset\n",
    "data_path = download_dataset(DATASET_CHOICE)\n",
    "if data_path:\n",
    "    print(f\"üìÅ Dataset available at: {data_path}\")\n",
    "    \n",
    "    # List the downloaded files\n",
    "    print(\"\\nüìÇ Downloaded files:\")\n",
    "    for f in Path(data_path).glob('*'):\n",
    "        print(f\"  - {f.name}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to download the dataset. Please check your internet connection and Kaggle credentials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e38fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo keep datasets small and reproducible:\\n- Use **random_state = 42** for deterministic sampling.\\n- Limit to **‚â§ 10 000 flows** for training.\\n- Select a balanced subset across classes (normal vs attack).\\n- Save subset to `/data/sample/flows.csv`.\\n\\nFor large CSVs: use `chunksize` loading in pandas to avoid memory overflow.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## üßÆ Downsampling & Sampling Strategy\n",
    "\"\"\"\n",
    "To keep datasets small and reproducible:\n",
    "- Use **random_state = 42** for deterministic sampling.\n",
    "- Limit to **‚â§ 10 000 flows** for training.\n",
    "- Select a balanced subset across classes (normal vs attack).\n",
    "- Save subset to `/data/sample/flows.csv`.\n",
    "\n",
    "For large CSVs: use `chunksize` loading in pandas to avoid memory overflow.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cf1ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved sample to data/sample/flows.csv, rows: 10000\n"
     ]
    }
   ],
   "source": [
    "# üìä Example: sample 10 000 records deterministically\n",
    "if DATASET_CHOICE == \"NSL-KDD\":\n",
    "    full_csv = Path(data_path) / \"KDDTrain+.txt\"\n",
    "    if full_csv.exists():\n",
    "        df = pd.read_csv(full_csv, header=None)\n",
    "        sample_df = df.sample(n=10000, random_state=42)\n",
    "        sample_df.to_csv(\"data/sample/flows.csv\", index=False)\n",
    "        print(\"‚úÖ Saved sample to data/sample/flows.csv, rows:\", sample_df.shape[0])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Dataset file not found. Check path:\", full_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec02578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## üß™ Synthetic Dataset (Scapy Recipe)\\nIf no public dataset is chosen or for quick tests, generate small, labeled synthetic flows.\\n\\nTraffic templates:\\n1. Normal web browsing (HTTP GETs)\\n2. DNS queries\\n3. IoT telemetry (UDP)\\n4. SYN flood bursts (DoS)\\n5. Suspicious payload injections\\n\\nEach sample includes:\\n- Flow ID, packet count, byte count, duration, avg packet size, flags, label.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## üß™ Synthetic Dataset (Scapy Recipe)\n",
    "If no public dataset is chosen or for quick tests, generate small, labeled synthetic flows.\n",
    "\n",
    "Traffic templates:\n",
    "1. Normal web browsing (HTTP GETs)\n",
    "2. DNS queries\n",
    "3. IoT telemetry (UDP)\n",
    "4. SYN flood bursts (DoS)\n",
    "5. Suspicious payload injections\n",
    "\n",
    "Each sample includes:\n",
    "- Flow ID, packet count, byte count, duration, avg packet size, flags, label.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86201676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Synthetic PCAP + flow CSV generated.\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_pcaps(num_flows=50, packets_per_flow=20):\n",
    "    flows_meta = []\n",
    "    packets = []\n",
    "    for i in range(num_flows):\n",
    "        label = \"normal\" if i % 5 != 0 else \"attack\"\n",
    "        src_ip = f\"192.168.0.{i%255}\"\n",
    "        dst_ip = f\"10.0.0.{(i*2)%255}\"\n",
    "        for j in range(packets_per_flow):\n",
    "            payload = b\"GET /index.html\" if label == \"normal\" else b\"GET /cmd?malicious=1\"\n",
    "            pkt = IP(src=src_ip, dst=dst_ip)/TCP(dport=80, sport=random.randint(1000,60000))/Raw(load=payload)\n",
    "            packets.append(pkt)\n",
    "        flows_meta.append({\n",
    "            \"flow_id\": i,\n",
    "            \"src_ip\": src_ip,\n",
    "            \"dst_ip\": dst_ip,\n",
    "            \"pkt_count\": packets_per_flow,\n",
    "            \"avg_pkt_size\": np.mean([len(p) for p in packets[-packets_per_flow:]]),\n",
    "            \"label\": label\n",
    "        })\n",
    "    wrpcap(\"data/sample/packets.pcap\", packets)\n",
    "    pd.DataFrame(flows_meta).to_csv(\"data/sample/flows.csv\", index=False)\n",
    "    print(\"‚úÖ Synthetic PCAP + flow CSV generated.\")\n",
    "\n",
    "generate_synthetic_pcaps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83c7cd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Check structure and small preview of sampled dataset.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ‚úÖ Verification of Sample Data\n",
    "\"\"\"Check structure and small preview of sampled dataset.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e364611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow_id</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>dst_ip</th>\n",
       "      <th>pkt_count</th>\n",
       "      <th>avg_pkt_size</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>192.168.0.0</td>\n",
       "      <td>10.0.0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>60.0</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>192.168.0.1</td>\n",
       "      <td>10.0.0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>192.168.0.2</td>\n",
       "      <td>10.0.0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>192.168.0.3</td>\n",
       "      <td>10.0.0.6</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>192.168.0.4</td>\n",
       "      <td>10.0.0.8</td>\n",
       "      <td>20</td>\n",
       "      <td>55.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flow_id       src_ip    dst_ip  pkt_count  avg_pkt_size   label\n",
       "0        0  192.168.0.0  10.0.0.0         20          60.0  attack\n",
       "1        1  192.168.0.1  10.0.0.2         20          55.0  normal\n",
       "2        2  192.168.0.2  10.0.0.4         20          55.0  normal\n",
       "3        3  192.168.0.3  10.0.0.6         20          55.0  normal\n",
       "4        4  192.168.0.4  10.0.0.8         20          55.0  normal"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/sample/flows.csv\")\n",
    "print(\"Rows:\", len(df))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77bd9076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Summary saved ‚Üí data/sample/dataset_summary.json\n"
     ]
    }
   ],
   "source": [
    "summary = {\n",
    "    \"dataset\": DATASET_CHOICE,\n",
    "    \"rows\": len(df),\n",
    "    \"columns\": list(df.columns),\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"source\": str(data_path) if data_path else \"synthetic\",\n",
    "}\n",
    "with open(\"data/sample/dataset_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "print(\"üìÑ Summary saved ‚Üí data/sample/dataset_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d3ed063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## üõ∞Ô∏è Live Capture Integration (Optional)\\nIf configured in `extras/live_capture_config.yaml`,  \\nrun `extras/live_capture.py` to collect short live traces for testing.\\nThe capture will:\\n- Store packets to `/data/live_capture/`\\n- Append summary flows to `/data/sample/flows_live.csv`\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## üõ∞Ô∏è Live Capture Integration (Optional)\n",
    "If configured in `extras/live_capture_config.yaml`,  \n",
    "run `extras/live_capture.py` to collect short live traces for testing.\n",
    "The capture will:\n",
    "- Store packets to `/data/live_capture/`\n",
    "- Append summary flows to `/data/sample/flows_live.csv`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0234744d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## üèÅ Notebook Summary\\nYou have now prepared small datasets ready for preprocessing and feature extraction.\\n\\n**Next:**  \\n‚û°Ô∏è Proceed to [02_preprocessing_and_feature_engineering.ipynb](02_preprocessing_and_feature_engineering.ipynb)\\n\\n**Artifacts Created:**\\n- `data/sample/flows.csv`\\n- `data/sample/packets.pcap`\\n- `data/sample/dataset_summary.json`\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## üèÅ Notebook Summary\n",
    "You have now prepared small datasets ready for preprocessing and feature extraction.\n",
    "\n",
    "**Next:**  \n",
    "‚û°Ô∏è Proceed to [02_preprocessing_and_feature_engineering.ipynb](02_preprocessing_and_feature_engineering.ipynb)\n",
    "\n",
    "**Artifacts Created:**\n",
    "- `data/sample/flows.csv`\n",
    "- `data/sample/packets.pcap`\n",
    "- `data/sample/dataset_summary.json`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e47ad1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Acquisition Complete. Proceed to 02_preprocessing_and_feature_engineering.ipynb.\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Data Acquisition Complete. Proceed to 02_preprocessing_and_feature_engineering.ipynb.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d4f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
